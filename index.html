<html lang="en">

<head>    
    <!-- Google tag (gtag.js) -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=G-NLEVHY0HS1"></script>
	<script>
 	window.dataLayer = window.dataLayer || [];
  	function gtag(){dataLayer.push(arguments);}
  	gtag('js', new Date());

  	gtag('config', 'G-NLEVHY0HS1');
	</script>

    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css"
        integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">
    <!-- Custom styles for this template -->
    <link href="files/jumbotron.css" rel="stylesheet">
    <script src="js/main.js"></script>
    <script src="js/scroll.js"></script>
    <title>Jingdong Zhang</title>
</head>



<body>
    <div class="container">
        <h3 id="Home" style="padding-top: 80px; margin-top: -80px;"></h3>
    </div>
    <nav class="navbar navbar-expand-md navbar-dark fixed-top bg-dark" id="Home">
        <div class="container">
            <a class="navbar-brand" href="#Home">Jingdong Zhang (张靖东)</a>

            <div class="collapse navbar-collapse" id="navbarToggle">
                <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="#Home">Home</a>
                    </li>
                    <li class="nav-item">
                        <a class="nav-link" href="#Publications">Publications</a>
                    </li>
                </ul>
            </div>
        </div>
    </nav>

    <div class="container" style="padding-top: 80px; font-size: 17px">
        <div class="row">
            <div class="col-md-4" ,="" style="padding-right: 40px">
                <br>
                <img class="img-responsive img-rounded" src="self_web.jpg" alt="Photo"
                    style="max-width: 100%; border:1px solid black">
            </div>

            <div class="col-md-8">
                <br>
                <p> I am currently a second-year Ph.D. student majoring in Computer Science at <a href="https://www.tamu.edu/index.html" target="_blank"> Texas A&M University </a>, 
			and my advisors are Prof. <a href="https://scholar.google.com/citations?user=28shvv0AAAAJ&hl=en" target="_blank">
			Wenping Wang</a> and Prof. <a href="https://people.tamu.edu/~xinli/" target="_blank">Xin Li</a>. I received the Bachelor of Engineering degree at <a href="https://www.fudan.edu.cn/en/" target="_blank"> Fudan University </a> in 2023. 
			I also worked as a research assistant at <a href="https://cse.hkust.edu.hk/" target="_blank">HKUST CSE</a> department with Prof. <a href="https://www.danxurgb.net/"
                        target="_blank">Dan Xu</a>. Previously, I have been working with Prof. <a href="http://www.it.fudan.edu.cn/En/Data/View/3012" target="_blank">Tao Chen</a> 
			and Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=gsLd2ccAAAAJ" target="_blank">Jiayuan Fan</a> at Fudan University.
                </p>

                <p> My research interests lie in computer vision and graphics. For computer graphics, I work on 3D asset generation and neural rendering. 
			For computer vision, I work on representation learning and scene understanding especially under multi-tasking scenarios.
                </p>

                <p>
                    Email: jdzhang [at] tamu [dot] edu <br>
		    Wechat: Triumph0929 <br>
                    <a href="https://openreview.net/profile?id=~Jingdong_Zhang2" target="_blank">OpenReview</a> /
                    <a href="https://github.com/Evergreen0929" target="_blank">GitHub</a> / 
		    <a href="https://scholar.google.com/citations?user=bhcm3NEAAAAJ&hl=zh-CN" target="_blank">Google Scholar</a> / 
 		    <a href="https://drive.google.com/file/d/1wCKM9wosuzSTlGWINfuFeb1m0sTNzOEM/view?usp=drive_link" target="_blank">CV</a> 
			<br>
                </p>

            </div>
        </div>
    </div><br>


    <!-- News -->
    <div class="container">
        <h3 id="News" style="padding-top: 80px; margin-top: -80px;">News</h3>
        <ul>
            <!--
			<li>
				<font color="firebrick">[Upcoming]</font> Invited Talk at <b>Carnegie Mellon University</b>.
			</li>
			-->
	    <li> <b style="color: darkred;">Aug. 2025</b>: SPGen has been accepted by SIGGRAPH Aisa 2025!</li>
		<li> <b>May. 2025</b>: Start the research internship in Adobe!
	    <li> <b>Jan. 2025</b>:  HiTTs has been accepted by ACM MM 2025!
		<li> <b>Jan. 2025</b>:  BridgeNet has been accepted by TPAMI!
            <li> <b>Jul. 2024</b>: I have been invited to serve as a reviewer for Pacific Graphics.
	    <li> <b>May. 2024</b>: Start the research internship in Tecent America!
            <li> <b>Nov. 2023</b>: I have been invited to serve as a reviewer for CVPR2024.
	    <!-- <li> <b>Aug. 2023</b>: A co-authored paper has been accepted by TIP! -->
	    <li> <b>Nov. 2022</b>: I have been invited to serve as a reviewer for CVPR2023.
		<!-- <li> <b>Sep. 2022</b>: I am honored to won the third prize of Excellent Undergraduate Scholarship of Fudan University. -->
	    <li> <b>Mar. 2022</b>: I have been invited to serve as a reviewer for ECCV2022.
	    <li> <b>Nov. 2021</b>: I have been invited to serve as a reviewer for CVPR2022.
		<!-- <li> <b>May. 2021</b>: I served as the team leader in Advanced Driving Assistance System (ADAS) National Competition, and our team won the first price! -->
		<!-- <li> <b>Sep. 2020</b>: I am honored to won the second prize of Excellent Undergraduate Scholarship of Fudan University. -->
        </ul>
    </div><br><br>


    <!-- Education -->
    <div class="container">
        <h3 id="Education" stype="padding-top: 80px; margin-top: -80px;">
            Education
        </h3>

        <hr>

        <div class="row">
            <div class="col-md-3">
                <a href="https://www.tamu.edu/index.html" target="_blank"><img class="img-fluid img-rounded"
                        src="TAMU.png" alt=""></a>
            </div>

            <div class="col-md-9">
                <b>
                    <font color="black">Texas A&M University</font>
                </b>
                <br>
                <!-- No. 220, Handan Road, Yangpu District, Shanghai, People's Republic of China<br> -->
                Department of Computer Science and Engineering<br>
		Ph.D Student<br>
                Auguest 2023 - recent
            </div>
        </div>
        <hr>

	<div class="row">
            <div class="col-md-3">
                <a href="https://www.fudan.edu.cn/en/" target="_blank"><img class="img-fluid img-rounded"
                        src="fudan.png" alt=""></a>
            </div>

            <div class="col-md-9">
                <b>
                    <font color="black">Fudan University</font>
                </b>
                <br>
                Intelligent Science and Technology (excellent class), School of Information Science and Engineering<br>
                Undergraduate Student<br>
                September 2019 - June 2023, received the bachelor's degree in Jun 2023
            </div>
        </div>
        <hr>
    </div><br><br>

    <!-- Internship -->
    <div class="container">
        <h3 id="Education" stype="padding-top: 80px; margin-top: -80px;">
            Internship
        </h3>

        <hr>

	<div class="row">
            <div class="col-md-3">
                <a href="https://www.tencent.com/en-us/about.html" target="_blank"><img class="img-fluid img-rounded"
                        src="tencent_logo.jpg" alt=""></a>
            </div>

            <div class="col-md-9">
                <b>
                    <font color="black">Tencent America</font>
                </b>
		<br>
                Research Intern, working on high-quality 3D asset generation.<br>
                May 2024 - Aug 2024
            </div>
        </div>
        <hr>
	<div class="row">
            <div class="col-md-3">
                <a href="https://www.adobe.com/" target="_blank"><img class="img-fluid img-rounded"
                        src="adobe_logo.jpg" alt=""></a>
            </div>

            <div class="col-md-9">
                <b>
                    <font color="black">Adobe</font>
                </b>
		<br>
                Research Intern, working on generative soft inpainting.<br>
                May 2025 - Aug 2025
            </div>
        </div>
        <hr>
    </div><br><br>



    <!-- Publications -->
    <div class="container">
        <h3 id="Publications" style="padding-top: 80px; margin-top: -80px;">
            Publications
        </h3>

        <hr>
            <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid img-rounded" src="representative_img.jpg" alt="">
                </div>
                <div class="col-md-9">
					<a href="https://evergreen0929.github.io/" target="_blank"><b>Jingdong Zhang</b></a>,
					<a href="https://chenweikai.github.io/" target="_blank"><b>Weikai Chen</b></a>,
                    <a href="https://liuyuan-pal.github.io/" target="_blank">Yuan Liu</a>,
					<a href="https://shanemankiw.github.io/" target="_blank">Jionghao Wang</a>,
					<a href="https://yzmblog.github.io/" target="_blank">Zhengming Yu</a>,
                    <a href="https://mickshen7558.github.io/" target="_blank">Zhuowen Shen</a>,
                    <a href="https://sites.google.com/site/boyanghome/home" target="_blank">Bo Yang</a>,
		    		<a href="https://scholar.google.com/citations?user=28shvv0AAAAJ&hl=en" target="_blank">Wenping Wang</a>,
					<a href="https://people.tamu.edu/~xinli/" target="_blank">Xin Li</a>
                    <br>
                    <b>
                        <font color="black"> SPGen: Spherical Projection as Consistent and Flexible Representation for Single Image 3D Shape Generation</font>
                    </b><br>
					<b><a href="https://asia.siggraph.org/2025/" target="_blank">SIGGRAPH Asia</a></b>, 2025 &nbsp&nbsp 
					[<a href="https://arxiv.org/abs/2509.12721" target="_blank"> <i>arxiv</i> </a>]
					<br>
			    	<i> SPGen leverages Spherical Projection (SP) to generate high-quality 3D shapes with i）Consistency: SP maps ensure view-consistent and unambiguous 3D reconstruction, 
				    ii) Flexibility: Supports arbitrary topologies, iii) Efficiency: Inherit powerful 2D diffusion priors and enables efficient finetuning.</i>
                </div>
            </div>


        <hr>
            <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid img-rounded" src="SolidGS.gif" alt="">
                </div>
                <div class="col-md-9">
                    <a href="https://mickshen7558.github.io/" target="_blank">Zhuowen Shen</a>,
                    <a href="https://liuyuan-pal.github.io/" target="_blank">Yuan Liu</a>,
                    <a href="https://zhangchen8.github.io/" target="_blank">Zhang Chen</a>,
                    <a href="https://sites.google.com/site/lizhong19900216/" target="_blank">Zhong Li</a>,
                    <a href="https://jiepengwang.github.io/" target="_blank">Jiepeng Wang</a>,
                    <a href="https://lyq.me/scholar" target="_blank">Yongqing Liang</a>,
                    <a href="https://yzmblog.github.io/" target="_blank">Zhengming Yu</a>,
                    <a href="https://evergreen0929.github.io/" target="_blank"><b>Jingdong Zhang</b></a>,
                    <a href="https://scholar.google.com/citations?user=ldanjkUAAAAJ&hl=en" target="_blank">Yi Xu</a>,
                    <a href="https://people.engr.tamu.edu/schaefer/index.html" target="_blank">Scott Schaefer</a>,
	            <a href="https://people.tamu.edu/~xinli/" target="_blank">Xin Li</a>,
		    <a href="https://scholar.google.com/citations?user=28shvv0AAAAJ&hl=en" target="_blank">Wenping Wang</a>
                    <br>
                    <b>
                        <font color="black"> SolidGS: Consolidating Gaussian Surfel Splatting for Sparse-View Surface Reconstruction </font>
                    </b><br>
                    Arxiv, 2024 &nbsp&nbsp  [<a href="https://arxiv.org/abs/2412.15400" target="_blank"> <i>arxiv</i> </a>]  &nbsp&nbsp 
                    [<a href="https://mickshen7558.github.io/projects/SolidGS/" target="_blank"> <i>project</i> </a>]<br>
			    <i><b>Abstract:</b> We present SolidGS, which reconstructs a consolidated Gaussian field from sparse inputs. Given only three input views, 
                    our approach enables high-precision and detailed mesh extraction, and high-quality novel view synthesis, achieved within just three minutes.</i>
                </div>
            </div>

	    <hr>
            <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid img-rounded" src="cvpr2024.png" alt="">
                </div>
                <div class="col-md-9">
                    <a href="https://evergreen0929.github.io/" target="_blank"><b>Jingdong Zhang</b></a>,
                    <a href="https://sites.google.com/site/yhrspace/" target="_blank">Hanrong Ye</a>,
		    <a href="https://people.tamu.edu/~xinli/" target="_blank">Xin Li</a>,
		    <a href="https://scholar.google.com/citations?user=28shvv0AAAAJ&hl=en" target="_blank">Wenping Wang</a>,
                    <a href="https://www.danxurgb.net/" target="_blank">Dan Xu</a>
                    <br>
                    <b>
                        <font color="black"> Multi-Task Label Discovery via Hierarchical Task Tokens for Partially Annotated Dense Predictions </font>
                    </b><br>
                    <b><a href="https://acmmm2025.org/" target="_blank">ACMMM (ACM MultiMedia)</a></b>, 2025 &nbsp&nbsp 
                    [<a href="https://arxiv.org/abs/2411.18823" target="_blank"> <i>arxiv</i> </a>]
			&nbsp&nbsp [<a href="https://github.com/Evergreen0929/EEMTL"> <i>code</i> </a>]<br>
			    <i><b>Abstract:</b> This research proposes a new approach to multi-task dense predictions with partially labeled data. We introduce hierarchical task tokens 
				 (HiTTs) to capture multi-level representations. The global task tokens conduct cross-task interactions and transfer 
				 knowledge from labeled to unlabeled tasks, while the fine-grained task tokens form high-quality task predictions at a finer granularity.</i>
                </div>
            </div>
	    <hr>
	    
	        <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid img-rounded" src="pami.png" alt="">
                </div>
                <div class="col-md-9">
                    <a href="https://evergreen0929.github.io/" target="_blank"><b>Jingdong Zhang</b></a>,
		            <a href="https://scholar.google.com/citations?hl=zh-CN&user=gsLd2ccAAAAJ" target="_blank">Jiayuan Fan</a>,
                    <a href="https://scholar.google.com/citations?user=UEZZP5QAAAAJ&hl=en&oi=sra" target="_blank">Peng
                        Ye</a>,
                    <a href="https://openreview.net/profile?id=~Bo_Zhang17" target="_blank">Bo Zhang</a>,
                    <a href="https://openreview.net/profile?id=~Hancheng_Ye1" target="_blank">Hancheng Ye</a>,
                    <a href="https://scholar.google.com/citations?user=OOY-4CwAAAAJ&hl=en" target="_blank">Baopu Li</a>,
                    <a href="https://caiyancheng.github.io/" target="_blank">Yancheng Cai</a>,
                    <a href="http://www.it.fudan.edu.cn/En/Data/View/3012" target="_blank">Tao Chen</a>
                    <br>
                    <b>
                        <font color="black"> BridgeNet: Comprehensive and Effective Feature Interactions via Bridge Feature for Multi-task Dense Predictions</font>
                    </b><br>
                    <b> <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">TPAMI (IEEE
                            Transactions on Pattern Analysis and Machine Intelligence)</a></b>, 2025 &nbsp&nbsp 
			    [<a href="https://ieeexplore.ieee.org/abstract/document/10870147" target="_blank"> <i>paper</i> </a>]
			    &nbsp&nbsp [<a href="https://github.com/Evergreen0929/EEMTL"> <i>code</i> </a>]<br>
                    <i><b>Abstract:</b> This work introduces a novel BridgeNet for multi-task learning on dense predictions. 
			    It uses a Bridge Feature Extractor (BFE) to create strong bridge features and a Task Pattern Propagation (TPP) to solve the task-pattern entanglement issue, 
			    which results in task-specific features with higher quality and discriminative task representations. 
			    A Task-Feature Refiner (TFR) is then used to refine the final task predictions.</i>
                </div>
            </div>
        <hr>

            <div class="row">
                <div class="col-md-3">
                    <img class="img-fluid img-rounded" src="tip.png" alt="">
                </div>
                <div class="col-md-9">
                    <a href="https://caiyancheng.github.io/" target="_blank">Yancheng Cai</a>,
                    <a href="https://openreview.net/profile?id=~Bo_Zhang17" target="_blank">Bo Zhang</a>,
                    <a href="https://scholar.google.com/citations?user=OOY-4CwAAAAJ&hl=en" target="_blank">Baopu Li</a>,
                    <a href="http://www.it.fudan.edu.cn/En/Data/View/3012" target="_blank">Tao Chen</a>,
                    <a href="https://scholar.google.com/citations?user=Obo7-bIAAAAJ&hl=en" target="_blank">Hongliang Yan</a>,
                    <a href="https://evergreen0929.github.io/" target="_blank"><b>Jingdong Zhang</b></a>,
                    Jiahao Xu
                    <br>
                    <b>
                        <font color="black">Rethinking Cross-Domain Pedestrian Detection: A Background-Focused
                            Distribution Alignment Framework for Instance-Free One-Stage Detectors
                        </font>
                    </b><br>
                    <b><a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank">TIP (IEEE
                            Transactions on Image Processing)</a></b>, 2023 &nbsp&nbsp [<a href="https://ieeexplore.ieee.org/document/10231122"  
			target="_blank"> <i>paper</i> </a>] &nbsp&nbsp [<a href="https://github.com/caiyancheng/BFDA"> <i>code</i> </a>] <br>
                    <i><b>Abstract:</b> We introduce a new approach for cross-domain pedestrian one-stage detectors. The paper identifies 
			    a foreground-background misalignment issue in image-level feature alignment, and a novel framework, Background-Focused 
			    Distribution Alignment (BFDA) is proposed to address this issue.</i>
                </div>
            </div>
            <hr>

        </div>
    </div><br><br>


    <!-- Research Experience -->
    <div class="container">
        <h3 id="Research Experience" style="padding-top: 80px; margin-top: -80px;">Research Experience</h3>
        <ul>
            <li>
                Jun. 2023 - Present, Ph.D. student, Aggie Graphics Group, Texas A&M University<br>
                Advisor: Prof. <a href="https://scholar.google.com/citations?user=28shvv0AAAAJ&hl=en" target="_blank">
			Wenping Wang</a> and Prof. <a href="https://people.tamu.edu/~xinli/" target="_blank">Xin Li</a>
            </li>
	
	    <li>
                Feb. 2022 - Present, Research Assistant, HKUST<br>
                Advisor: Prof. <a href="https://www.danxurgb.net/" target="_blank">Dan Xu</a>
            </li>
		
            <li>
                Jul. 2021 - Present, Research Assistant, Fudan Embedded Deep Learning and Visual Analysis Lab<br>
                Advisor: Prof. <a href="http://www.it.fudan.edu.cn/En/Data/View/3012" target="_blank">Tao Chen</a> 
		    and Prof. <a href="https://scholar.google.com/citations?hl=zh-CN&user=gsLd2ccAAAAJ" target="_blank">Jiayuan Fan</a>
            </li>
        </ul>
    </div><br><br>


    <!-- Award -->
    <div class="container">
        <h3 id="Award" style="padding-top: 80px; margin-top: -80px;">Selected Award</h3>
        <ul>
            <li>The third prize of outstanding Undergraduate Student Scholarship of Fudan University in 2021-2022 Academic year.</li>
            <li>The second prize of outstanding Undergraduate Student Scholarship of Fudan University in 2019-2020 Academic year.</li>
            <li>Outstanding Student of Fudan University in 2019-2020 Academic year.</li>
	    <li>The first prize of Advanced Driving Assistance System (ADAS) National Competition by Dell Corporation, May 2021.</li>
        </ul>
	<!--
	<hr>
            <div class="row">
                <div class="col-md-5">
                    <img class="img-fluid img-rounded" src="dell.png" alt="">
                </div>
                <div class="col-md-7">
		    <ul>
			<li><font color="black">The <b>first prize</b> of Advanced Driving Assistance System (ADAS) National Competition by Dell Corporation, 
				May 2021.</font>
                        <li>I am the team leader.
	 	    <ul>
                </div>
            </div>
	    <hr>
	-->
    </div><br><br>


    <!-- Academic Service -->
    <div class="container">
        <h3 id="Academic_Service" style="padding-top: 80px; margin-top: -80px;">Academic Service</h3>
        <ul>
            <li>Reviewer for: <br>
		    CVPR: 2022-2024, <br>
		    ECCV: 2022, <br>
		    ICRA: 2024 <br>
		    Pacific Graphics: 2024 <br>
        </ul>
    </div><br>

    <div class="container">
        <center>
		<hr>
		<div class="col-md-10">
		    <img class="img-fluid img-rounded" src="p1.jpg" alt="">
		</div>
		<i>TX, US</i>
		
		<hr>
		<div class="col-md-10">
		    <img class="img-fluid img-rounded" src="p2.jpg" alt="">
		</div>
		<i>Reflection Lake, Mount Rainier NP, WA, US</i>
		
		<hr>
		<div class="col-md-10">
		    <img class="img-fluid img-rounded" src="p3.jpg" alt="">
		</div>
		<i>AZ, US</i>

        <hr>
		<div class="col-md-10">
		    <img class="img-fluid img-rounded" src="p4.jpg" alt="">
		</div>
		<i>Petrified Forest NP, AZ, US</i>

		<hr>
		
            <footer>
                <p>© Jingdong Zhang 2024</p>
            </footer>
        </center>
    </div>
    <!-- /container -->

    <!-- Bootstrap core JavaScript -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script>showPubs(1);</script>
    <script>var scroll = new SmoothScroll('a[href*="#"]', { speed: 1000 });</script>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js"
        integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4"
        crossorigin="anonymous"></script>
</body>

</html>
